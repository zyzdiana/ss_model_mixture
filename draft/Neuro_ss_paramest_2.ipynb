{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "import pyparticleest.utils.kalman as kalman\n",
    "import pyparticleest.interfaces as interfaces\n",
    "import pyparticleest.paramest.paramest as param_est\n",
    "import pyparticleest.paramest.interfaces as pestint\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset(steps, P0, Q):\n",
    "    x = numpy.zeros((steps + 1,))\n",
    "    y = numpy.zeros((steps + 1,))\n",
    "    x[0] = numpy.random.multivariate_normal((0.0,), P0)\n",
    "    y[0] = numpy.random.binomial(1,numpy.exp(x[0])/(1.0+numpy.exp(x)[0]))\n",
    "    for k in range(0, steps):\n",
    "        x[k + 1] = x[k] + numpy.random.multivariate_normal((0.0,), Q)\n",
    "        y[k + 1] = bernoulli.rvs(numpy.exp(x[k+1])/(1.0 + numpy.exp(x[k+1])))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmean(logw, val):\n",
    "    w = numpy.exp(logw)\n",
    "    w = w / sum(w)\n",
    "    return numpy.sum(w * val.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(interfaces.FFBSiRS, interfaces.ParticleFiltering,\n",
    "            pestint.ParamEstInterface, pestint.ParamEstBaseNumeric):\n",
    "    \"\"\" x_{k+1} = x_k + v_k, v_k ~ N(0,Q),\n",
    "        y_k = e_k, e_k ~ Bernoulli(\\exp(x_{k})/(1+\\exp(x_{k}))),\n",
    "        x(0) ~ N(0,P0) \"\"\"\n",
    "\n",
    "    def __init__(self, P0, Q):\n",
    "        self.P0 = numpy.copy(P0)\n",
    "        self.Q = numpy.copy(Q)\n",
    "        self.logxn_max = kalman.lognormpdf_scalar(numpy.zeros((1,)), self.Q)\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def create_initial_estimate(self, N):\n",
    "        return numpy.random.normal(0.0, numpy.sqrt(self.P0), (N,))\n",
    "\n",
    "    def sample_process_noise(self, particles, u, t):\n",
    "        \"\"\" Return process noise for input u \"\"\"\n",
    "        N = len(particles)\n",
    "        return numpy.random.normal(0.0, numpy.sqrt(self.Q), (N,))\n",
    "\n",
    "    def update(self, particles, u, noise, t):\n",
    "        \"\"\" Update estimate using 'data' as input \"\"\"\n",
    "        particles[:] = particles + noise\n",
    "\n",
    "    def measure(self, particles, y, t):\n",
    "        \"\"\" Return the log-pdf value of the measurement \"\"\"\n",
    "        return bernoulli.logpmf(y,numpy.exp(particles)/(1.0 + numpy.exp(particles)))\n",
    "\n",
    "    def logp_xnext(self, particles, next_part, u, t):\n",
    "        \"\"\" Return the log-pdf value for the possible future state 'next' given input u \"\"\"\n",
    "        pn = particles\n",
    "        return kalman.lognormpdf_scalar(pn.ravel() - next_part.ravel(), self.Q)\n",
    "    \n",
    "    def logp_xnext_max(self, particles, u, t):\n",
    "        return self.logxn_max\n",
    "\n",
    "    def sample_smooth(self, part, ptraj, anc, future_trajs, find, ut, yt, tt, cur_ind):\n",
    "        \"\"\" Update ev. Rao-Blackwellized states conditioned on \"next_part\" \"\"\"\n",
    "        return part.reshape((-1, 1))\n",
    "\n",
    "    def set_params(self, params):\n",
    "        \"\"\" New set of parameters for which the integral approximation terms will be evaluated\"\"\"\n",
    "        self.params = numpy.copy(params)\n",
    "        self.Q = math.exp(params[0]) * numpy.eye(1)\n",
    "\n",
    "    def eval_logp_x0(self, particles, t):\n",
    "        \"\"\" Calculate gradient of a term of the I1 integral approximation\n",
    "            as specified in [1].\n",
    "            The gradient is an array where each element is the derivative with\n",
    "            respect to the corresponding parameter\"\"\"\n",
    "        return kalman.lognormpdf_scalar(particles, self.P0)\n",
    "\n",
    "    def copy_ind(self, particles, new_ind=None):\n",
    "        if (new_ind is not None):\n",
    "            return numpy.copy(particles[new_ind])\n",
    "        else:\n",
    "            return numpy.copy(particles)\n",
    "\n",
    "    def eval_logp_xnext_fulltraj(self, straj, ut, tt):\n",
    "        part = straj.get_smoothed_estimates()\n",
    "        M = part.shape[1]\n",
    "        xp = part\n",
    "        diff = part[1:] - xp[:-1]\n",
    "        logp = kalman.lognormpdf_scalar(diff.ravel(), self.Q)\n",
    "        return numpy.sum(logp) / M\n",
    "    \n",
    "    def eval_logp_y_fulltraj(self, straj, yt, tt):\n",
    "        sest = straj.get_smoothed_estimates()\n",
    "        M = sest.shape[1]\n",
    "        yp = numpy.repeat(numpy.asarray(yt, dtype=float).reshape((-1, 1, 1)),\n",
    "                                 repeats=M, axis=1)\n",
    "        #return numpy.sum(kalman.lognormpdf_scalar(diff.ravel(), self.R)) / M\n",
    "        return numpy.sum(bernoulli.logpmf(yp.ravel(), \n",
    "                                          numpy.exp(sest.ravel())/(1.0 + numpy.exp(sest.ravel())))) / M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(params, Q, cur_iter):\n",
    "    params_it[cur_iter] = params\n",
    "    Q_it[cur_iter] = Q\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "        \n",
    "    for i in xrange(len(params)):\n",
    "        plt.plot(range(cur_iter + 1), params_it[:cur_iter + 1, i], '-')\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "    return (cur_iter >= len(iterations))\n",
    "\n",
    "\n",
    "def callback_sim(estimator):\n",
    "    # vals = numpy.empty((num, steps+1))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "#    mvals = numpy.empty((steps+1))\n",
    "#    for k in range(steps+1):\n",
    "#        #vals[:,k] = numpy.copy(estimator.pt.traj[k].pa.part)\n",
    "#        mvals[k] = wmean(estimator.pt.traj[k].pa.w,\n",
    "#                          estimator.pt.traj[k].pa.part)\n",
    "#        #plt.plot((k,)*num, vals[:,k], 'k.', markersize=0.5)\n",
    "#    plt.plot(range(steps+1), mvals, 'k-')\n",
    "\n",
    "\n",
    "    sest_mean = estimator.get_smoothed_mean()\n",
    "    for k in range(sest_mean.shape[1]):\n",
    "        plt.plot(range(steps + 1), sest_mean[:, k], 'g-')\n",
    "\n",
    "    plt.plot(range(steps + 1), x, 'r-')\n",
    "    plt.plot(range(steps + 1), y, 'bx')\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(params, Q, cur_iter):\n",
    "    print \"params = %s\" % numpy.exp(params)\n",
    "\n",
    "\n",
    "def callback_sim(estimator):\n",
    "    # vals = numpy.empty((num, steps+1))\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "#    mvals = numpy.empty((steps+1))\n",
    "#    for k in range(steps+1):\n",
    "#        #vals[:,k] = numpy.copy(estimator.pt.traj[k].pa.part)\n",
    "#        mvals[k] = wmean(estimator.pt.traj[k].pa.w,\n",
    "#                          estimator.pt.traj[k].pa.part)\n",
    "#        #plt.plot((k,)*num, vals[:,k], 'k.', markersize=0.5)\n",
    "#    plt.plot(range(steps+1), mvals, 'k-')\n",
    "\n",
    "\n",
    "    sest_mean = estimator.get_smoothed_mean()\n",
    "    for k in range(sest_mean.shape[1]):\n",
    "        plt.plot(range(steps + 1), sest_mean[:, k], 'g-')\n",
    "\n",
    "    plt.plot(range(steps + 1), x, 'r-')\n",
    "    plt.plot(range(steps + 1), y, 'bx')\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    numpy.random.seed(1)\n",
    "    steps = 49\n",
    "    iterations = numpy.asarray(range(500))\n",
    "    num = numpy.ceil(500 + 4500.0 / (iterations[-1] ** 3) * iterations ** 3).astype(int)\n",
    "    M = numpy.ceil(50 + 450.0 / (iterations[-1] ** 3) * iterations ** 3).astype(int)\n",
    "    P0 = 2.0 * numpy.eye(1)\n",
    "    Q = 1.0 * numpy.eye(1)\n",
    "    (x, y) = generate_dataset(steps, P0, Q)\n",
    "    theta0 = numpy.log(numpy.asarray((2.0,)))\n",
    "    \n",
    "    params_it = numpy.zeros((len(iterations) + 1, 2))\n",
    "    Q_it = numpy.zeros((len(iterations) + 1))\n",
    "\n",
    "    theta_true = numpy.asarray((1.0,))\n",
    "    \n",
    "    model = Model(P0, Q)\n",
    "    estimator = param_est.ParamEstimation(model, u=None, y=y)\n",
    "    plt.ion()\n",
    "    callback(theta0, None, -1)\n",
    "    param=estimator.maximize(theta0, num, M, smoother='full', meas_first=True,\n",
    "                       max_iter=len(iterations),callback=callback)\n",
    "    plt.ioff()\n",
    "    callback(param, None, len(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
