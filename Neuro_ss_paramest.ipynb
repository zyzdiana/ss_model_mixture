{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation\n",
    "## Particle Smoother EM (PSEM)\n",
    "### M-Step: maximize the following Monte Carlo approximation with respect to  $\\theta$\n",
    "\n",
    "$\\hat{Q}_r(\\theta) = \\frac{1}{M_r} \\sum_{j=1}^{M_r} \\log p(\\tilde{x}_{1:T}^j, y_{1:t};\\theta)$\n",
    "\n",
    "where\n",
    "\n",
    "$M_r$: num. of backward trajectories\n",
    "\n",
    "$\\{ \\tilde{x}_{1:T}^j \\}_{j=1}^{M_r}$: $p_{\\theta[r-1]}(x_{1:T}|y_{1:T})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For our 1-dimensional state space model\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      x_k = x_{k-1} + \\epsilon_k, \\epsilon_k \\sim N(0,\\sigma^2_{\\epsilon K}) & (a) \\\\\n",
    "      \\lambda_r(k\\Delta)\\Delta = \\frac{e^{x_k}}{1+e^{x_k}} & (b)\\\\\n",
    "      \\Delta N_{k} | x_k \\sim Bernoulli(\\lambda_r(k\\Delta)\\Delta) & (c)\n",
    "\\end{array} \n",
    "\\right. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Data Likelihood\n",
    "\\begin{align*}\n",
    "p(N_{0,K}, x; \\theta) &= p(N_{0,K}|\\boldsymbol{x})p(\\boldsymbol{x})\\\\\n",
    "&= \\prod\\limits_{k=1}^{K}\\left\\{p(\\Delta N_{k}|x_k)\\right\\}\\prod\\limits_{k=1}^{K} p(x_k|x_{k-1}; \\sigma_{\\epsilon K}^2)\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\\begin{align*}\n",
    "p(x; \\theta) &= (2\\pi \\sigma_{\\epsilon K}^2)^{\\frac{-K}{2}} \\exp {\\left\\{-\\sum_{k=1}^{K} \\frac{(x_k-x_{k-1})^2}{2\\sigma_{\\epsilon K}^2}\\right\\}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "p(N_{0,K}|x) &= \\prod\\limits_{k=1}^{K} (\\lambda_r(k\\Delta)\\Delta )^{\\Delta N_{k}} (1-(\\lambda_r(k\\Delta)\\Delta))^{1-\\Delta N_{k}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step:\n",
    "\\begin{align*}\n",
    "Q(\\theta|\\theta^{l}) &= E\\left\\{\\log [p(N_{0,K}, x; \\theta)]|| H_K, \\theta^{(l)} \\right\\}\\\\\n",
    "&=\\int \\log p(N_{0,K}, x; \\theta) p(x|H_k,\\theta^{(l-1)}) dx\n",
    "\\end{align*}\n",
    "\n",
    "#### use Sequential Monte Carlo to sample backward smoother \n",
    "\n",
    "%% $E\\{x_k || H_k, \\theta^{(l)}\\}$, $W_k = E\\{x_k^2 || H_k, \\theta^{(l)}\\}$ and $W_{k,k+1}=E\\{x_k x_{k-1} || H_k, \\theta^{(l)}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step:\n",
    "\n",
    "$\\sigma_{\\epsilon K}^{2(l+1)} = K^{-1} [\\sum_{k=1}^K W_k +\\sum_{k=1}^K W_{k-1} -2\\sum_{k=1}^K W_{k,k-1}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import math\n",
    "import pyparticleest.utils.kalman as kalman\n",
    "import pyparticleest.interfaces as interfaces\n",
    "import pyparticleest.paramest.paramest as param_est\n",
    "import pyparticleest.paramest.interfaces as pestint\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "from scipy.stats import bernoulli\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset(steps, P0, Q):\n",
    "    x = numpy.zeros((steps + 1,))\n",
    "    y = numpy.zeros((steps + 1,))\n",
    "    x[0] = numpy.random.multivariate_normal((0.0,), P0)\n",
    "    y[0] = numpy.random.binomial(1,numpy.exp(x[0])/(1.0+numpy.exp(x)[0]))\n",
    "    for k in range(0, steps):\n",
    "        x[k + 1] = x[k] + numpy.random.multivariate_normal((0.0,), Q)\n",
    "        y[k + 1] = bernoulli.rvs(numpy.exp(x[k+1])/(1.0 + numpy.exp(x[k+1])))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmean(logw, val):\n",
    "    w = numpy.exp(logw)\n",
    "    w = w / sum(w)\n",
    "    return numpy.sum(w * val.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(interfaces.ParticleFiltering, interfaces.FFBSiRS, pestint.ParamEstInterface):\n",
    "    \"\"\" x_{k+1} = x_k + v_k, v_k ~ N(0,Q),\n",
    "        y_k = e_k, e_k ~ Bernoulli(\\exp(x_{k})/(1+\\exp(x_{k}))),\n",
    "        x(0) ~ N(0,P0) \"\"\"\n",
    "\n",
    "    def __init__(self, P0, Q):\n",
    "        self.P0 = numpy.copy(P0)\n",
    "        self.Q = numpy.copy(Q)\n",
    "        self.logxn_max = kalman.lognormpdf_scalar(numpy.zeros((1,)), self.Q)\n",
    "\n",
    "    def create_initial_estimate(self, N):\n",
    "        return numpy.random.normal(0.0, numpy.sqrt(self.P0), (N,))\n",
    "\n",
    "    def sample_process_noise(self, particles, u, t):\n",
    "        \"\"\" Return process noise for input u \"\"\"\n",
    "        N = len(particles)\n",
    "        return numpy.random.normal(0.0, numpy.sqrt(self.Q), (N,))\n",
    "\n",
    "    def update(self, particles, u, noise, t):\n",
    "        \"\"\" Update estimate using 'data' as input \"\"\"\n",
    "        particles[:] = particles + noise\n",
    "\n",
    "    def measure(self, particles, y, t):\n",
    "        \"\"\" Return the log-pdf value of the measurement \"\"\"\n",
    "        return bernoulli.logpmf(y,numpy.exp(particles)/(1.0 + numpy.exp(particles)))\n",
    "\n",
    "\n",
    "    def logp_xnext(self, particles, next_part, u, t):\n",
    "        \"\"\" Return the log-pdf value for the possible future state 'next' given input u \"\"\"\n",
    "        pn = particles\n",
    "        return kalman.lognormpdf_scalar(pn.ravel() - next_part.ravel(), self.Q)\n",
    "\n",
    "    def logp_xnext_max(self, particles, u, t):\n",
    "        return self.logxn_max\n",
    "\n",
    "    def sample_smooth(self, part, ptraj, anc, future_trajs, find, ut, yt, tt, cur_ind):\n",
    "        \"\"\" Update ev. Rao-Blackwellized states conditioned on \"next_part\" \"\"\"\n",
    "        return part.reshape((-1, 1))\n",
    "\n",
    "    def set_params(self, params):\n",
    "        \"\"\" New set of parameters for which the integral approximation terms will be evaluated\"\"\"\n",
    "        self.Q = params[0] * numpy.eye(1)\n",
    "\n",
    "    def eval_logp_x0(self, particles, t):\n",
    "        \"\"\" Calculate gradient of a term of the I1 integral approximation\n",
    "            as specified in [1].\n",
    "            The gradient is an array where each element is the derivative with\n",
    "            respect to the corresponding parameter\"\"\"\n",
    "        return kalman.lognormpdf_scalar(particles, self.P0)\n",
    "    \n",
    "    def copy_ind(self, particles, new_ind=None):\n",
    "        if (new_ind is not None):\n",
    "            return numpy.copy(particles[new_ind])\n",
    "        else:\n",
    "            return numpy.copy(particles)\n",
    "\n",
    "    def eval_logp_xnext_fulltraj(self, straj, ut, tt):\n",
    "        M = straj.shape[1]\n",
    "        part = straj\n",
    "        xp = part\n",
    "        diff = part[1:] - xp[:-1]\n",
    "        logp = kalman.lognormpdf_scalar(diff.ravel(), self.Q)\n",
    "        return numpy.sum(logp) / M\n",
    "\n",
    "    def eval_logp_y_fulltraj(self, straj, yt, tt):\n",
    "        M = straj.shape[1]\n",
    "        diff = numpy.repeat(numpy.asarray(yt, dtype=float).reshape((-1, 1, 1)),\n",
    "                                 repeats=M, axis=1)\n",
    "        return numpy.sum(bernoulli.logpmf(diff.ravel(), numpy.exp(straj.ravel())/(1.0 + numpy.exp(straj.ravel())))) / M\n",
    "\n",
    "    def maximize_weighted(self, straj, alltrajs, weights):\n",
    "        return self.maximize_weighted_analytic(straj, alltrajs, weights)\n",
    "        #return self.maximize_weighted_numeric(straj, alltrajs, weights)\n",
    "        \n",
    "    def maximize_weighted_analytic(self, straj, alltrajs, weights):\n",
    "        M = alltrajs.shape[1]\n",
    "\n",
    "        tt = straj.t\n",
    "        yt = straj.y\n",
    "        part = alltrajs\n",
    "\n",
    "        xp = part\n",
    "        diff = part[1:] - xp[:-1]\n",
    "\n",
    "        werr = numpy.empty((len(alltrajs) - 1, M))\n",
    "\n",
    "        for j in xrange(len(weights)):\n",
    "            werr[:, j:j + 1] = weights[j] * diff[:, j] ** 2\n",
    "\n",
    "        Q = numpy.mean(numpy.sum(werr, axis=1))\n",
    "\n",
    "        newparams = numpy.asarray((Q,))\n",
    "        #newparams = numpy.asarray((Q,))\n",
    "        return newparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    numpy.random.seed(1)\n",
    "    steps = 2000 #1499\n",
    "    max_iter = 3000\n",
    "    forwardnum = 1000\n",
    "    backwardnum = 400\n",
    "    P0 = 2.0 * numpy.eye(1)\n",
    "    Q = 1.0 * numpy.eye(1)\n",
    "\n",
    "    (x, y) = generate_dataset(steps, P0, Q)\n",
    "\n",
    "    t = numpy.arange(steps + 1)\n",
    "    \n",
    "    def callback_sim(estimator):\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        sest_mean = estimator.get_smoothed_mean()\n",
    "        for k in range(sest_mean.shape[1]):\n",
    "            plt.plot(range(steps + 1), sest_mean[:, k], 'g-', label= 'Smoother mean')\n",
    "\n",
    "        plt.plot(range(steps + 1), x, 'r-', label= 'true x')\n",
    "        plt.plot(range(steps + 1), y, 'bx', label= 'y')\n",
    "        plt.legend(loc='best')\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "\n",
    "    params_it = numpy.zeros((max_iter + 1, 2))\n",
    "    Q_it = numpy.zeros((max_iter + 1))\n",
    "\n",
    "    theta_true = numpy.asarray((1.0,))\n",
    "    \n",
    "    def callback(params, Q, cur_iter):\n",
    "        params_it[cur_iter] = params\n",
    "        Q_it[cur_iter] = Q\n",
    "\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        for i in xrange(len(theta_true)):\n",
    "            plt.plot((0.0, cur_iter + 1), (theta_true[i], theta_true[i]), 'k--', label= '\\theta_true')\n",
    "\n",
    "        for i in xrange(len(params)):\n",
    "            plt.plot(range(cur_iter + 1), params_it[:cur_iter + 1, i], '-', label= '\\theta_est')\n",
    "        plt.legend(loc='best')\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "\n",
    "        return (cur_iter >= max_iter)\n",
    "    \n",
    "    theta0 = numpy.asarray((2.0,))\n",
    "    #theta0 = numpy.asarray((2.0,))\n",
    "    model = Model(P0, Q)\n",
    "    estimator = param_est.ParamEstimationPSAEM(model, u=None, y=y)\n",
    "    plt.ion()\n",
    "    callback(theta0, None, 0)\n",
    "    estimator.simulate(forwardnum, backwardnum, filter='pf', smoother='full', meas_first=True)\n",
    "    ctraj = numpy.copy(estimator.straj.traj)\n",
    "    filter_options = {'cond_traj': ctraj}\n",
    "\n",
    "    param = estimator.maximize(theta0, forwardnum, filter='pf',smoother='full',\n",
    "                       meas_first=True, max_iter=max_iter,\n",
    "                       filter_options=filter_options,\n",
    "                       callback=callback,\n",
    "                       callback_sim=callback_sim)[0]\n",
    "    plt.ioff()\n",
    "    callback(param, None, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='figure_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='figure_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood(param,estimator,cur_iter):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    est_smooth = estimator.get_smoothed_estimates()\n",
    "    diff=numpy.diff(est_smooth[:,:,0]-est_smooth[:,:,0],axis=0)\n",
    "    logpx = kalman.lognormpdf_scalar(diff.ravel(), param)\n",
    "    logpx0 = kalman.lognormpdf_scalar(est_smooth[0,:,0].ravel(), P0)\n",
    "    lpx=numpy.sum(logpx) / backwardnum\n",
    "    lpx0=numpy.sum(logpx0) / backwardnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
